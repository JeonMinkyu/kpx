{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "makinatory-dawn의 makinatory를 symbolic link로 연결해서 사용해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import datetime\n",
    "import math\n",
    "pd.options.mode.chained_assignment = None\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor, plot_importance\n",
    "# regression models\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections.abc import Iterable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, Sampler\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kpx_forecast_merged = pd.read_pickle(os.path.abspath(os.path.join(os.getcwd(),'..','..','kpx','data','df_kpx_forecast_merged.pkl')))\n",
    "df_hk1_forecast_merged = pd.read_pickle(os.path.abspath(os.path.join(os.getcwd(),'..','..','kpx','data','df_hk1_forecast_merged.pkl')))\n",
    "df_hk2_forecast_merged = pd.read_pickle(os.path.abspath(os.path.join(os.getcwd(),'..','..','kpx','data','df_hk2_forecast_merged.pkl')))\n",
    "df_ss1_forecast_merged = pd.read_pickle(os.path.abspath(os.path.join(os.getcwd(),'..','..','kpx','data','df_ss1_forecast_merged.pkl')))\n",
    "df_ss2_forecast_merged = pd.read_pickle(os.path.abspath(os.path.join(os.getcwd(),'..','..','kpx','data','df_ss2_forecast_merged.pkl')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_kpx_forecast_merged.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self,data):\n",
    "        self.len = len(data)\n",
    "        scaler = MinMaxScaler()\n",
    "        \n",
    "        data_y = data[['Power Generation(kW)+0']]#,'Power Generation(kW)+1','Power Generation(kW)+2']].values\n",
    "        data_x = data.drop(['Power Generation(kW)+0','Power Generation(kW)+1','Power Generation(kW)+2','date','date(forecast)','datetime','datetime(forecast)','location'],axis=1).values\n",
    "        \n",
    "        data_x = scaler.fit_transform(data_x)\n",
    "        data_y = scaler.fit_transform(data_y)\n",
    "        \n",
    "        self.x_data = torch.from_numpy(data_x).float()\n",
    "        self.y_data = torch.from_numpy(data_y).float()\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    #, self.df_date.iloc[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    \n",
    "class DatasetManager:\n",
    "    def __init__(self, data, n_splits, transform=transforms.ToTensor()):\n",
    "        # n_splits = 5\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "        train_index = list(tscv.split(data))[n_splits-1][0] #train 0.8\n",
    "        test_index = list(tscv.split(data))[n_splits-1][1] #test 0.2\n",
    "\n",
    "        train_df = data.iloc[train_index].reset_index(drop=True)\n",
    "        test_df = data.iloc[test_index].reset_index(drop=True)\n",
    "\n",
    "        train_index_2 = list(tscv.split(train_df))[n_splits-1][0]\n",
    "        valid_index_2 = list(tscv.split(train_df))[n_splits-1][1]\n",
    "\n",
    "        train_df_2 = train_df.iloc[train_index_2].reset_index(drop=True)\n",
    "        valid_df = train_df.iloc[valid_index_2].reset_index(drop=True)\n",
    "\n",
    "        self.train_df = train_df_2.copy().reset_index(drop=True)\n",
    "        self.valid_df = valid_df.copy().reset_index(drop=True)\n",
    "        self.test_df = test_df.copy().reset_index(drop=True)\n",
    "        \n",
    "        self.train_dataset = Dataset(self.train_df)\n",
    "        self.valid_dataset = Dataset(self.valid_df)\n",
    "        self.test_dataset = Dataset(self.test_df)\n",
    "        \n",
    "\n",
    "    def get_loaders(self, batch_size):\n",
    "        return [\n",
    "            DataLoader(\n",
    "                self.train_dataset, batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=4\n",
    "            ),\n",
    "            DataLoader(\n",
    "                self.valid_dataset, batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=4 \n",
    "            ),\n",
    "            DataLoader(\n",
    "                self.test_dataset, batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=4 \n",
    "            )\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (C) Makinarocks, Inc - All Rights Reserved\n",
    "# Unauthorized copying of this file, via any medium is strictly prohibited\n",
    "# Proprietary and confidential\n",
    "# Written by Kim, Ki Hyun <khkim@makinarocks.ai>, January 2019\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as pyplot\n",
    "from scipy.stats import norm\n",
    "from PIL import Image\n",
    "\n",
    "from makinatory.modules import Loss\n",
    "from makinatory.models import VariationalAutoEncoder as VAE\n",
    "from makinatory.trainers import Trainer\n",
    "from makinatory.utils.common_utils import set_random_seeds\n",
    "from makinatory.utils.performance import get_auc_roc, get_auc_prc\n",
    "\n",
    "\n",
    "def main(df, n_splits):\n",
    "    input_size = df.shape[1] - 8 #feature 갯수\n",
    "    btl_size = 100\n",
    "\n",
    "    k = 10\n",
    "    gpu_id = 0\n",
    "    device_name = \"cpu\"#\"cuda:{}\".format(gpu_id)\n",
    "\n",
    "    loss_type = \"mse\"\n",
    "\n",
    "    n_epochs = 200\n",
    "    batch_size = 256\n",
    "    early_stop = -1\n",
    "    start_layer_index = 1\n",
    "    end_layer_index = 1\n",
    "    n_layers = 12\n",
    "\n",
    "    # 1. Get data & init DataLoader\n",
    "    dset_manager = DatasetManager(df, n_splits)\n",
    "    train_loader, valid_loader, test_loader = dset_manager.get_loaders(batch_size=batch_size)\n",
    "    #\n",
    "    # 2. Init Model\n",
    "    #\n",
    "    model = VAE(\n",
    "        input_size=input_size,\n",
    "        btl_size=btl_size,\n",
    "        recon_loss=Loss(loss_type, reduction=\"mean\"),\n",
    "        distribution='normal', k=k\n",
    "    )\n",
    "    model = model.to(device_name)\n",
    "\n",
    "    #\n",
    "    # 3. Train\n",
    "    #\n",
    "    trainer = Trainer(model=model, device_name=device_name)\n",
    "    trainer.train(\n",
    "        train_loader=train_loader,\n",
    "        valid_loader=valid_loader,\n",
    "        n_epochs=n_epochs,\n",
    "        verbose=2,\n",
    "        early_stop=early_stop\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # 4. Test\n",
    "    #\n",
    "    x_tilde = trainer.test(test_loader)\n",
    "    test_x, test_y = dset_manager.get_transformed_data(test_loader) \n",
    "    test_y = np.where(np.isin(test_y, seen_labels), False, True) #true/false로 binary(test_y가 seen_labels면 false, unseen(novel)이면 true)\n",
    "    scores = []\n",
    "\n",
    "    for _x_tilde, _test_x in zip(x_tilde.split(batch_size), test_x.split(batch_size)):\n",
    "        scores.append(\n",
    "            Loss(loss_type, reduction='mean')(\n",
    "                _x_tilde, _test_x.to(device_name)\n",
    "            ).mean(dim=-1)\n",
    "        )\n",
    "        \n",
    "    scores = torch.cat(scores)\n",
    "    scores = scores.cpu().numpy()\n",
    " \n",
    "    auc_roc = get_auc_roc(test_y, scores)\n",
    "    auc_prc = get_auc_prc(test_y, scores)\n",
    "\n",
    "#     print('AUC ROC', auc_roc)\n",
    "#     print('AUC PRC', auc_prc)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    set_random_seeds(2019)\n",
    "    df_kpx = pd.read_pickle(os.path.abspath(os.path.join(os.getcwd(),'..','..','kpx','data','wind','df_kpx_forecast_merged.pkl')))\n",
    "    df_kpx = df_kpx.fillna(0)\n",
    "    df_kpx['Power Generation(kW)+0'] = df_kpx['Power Generation(kW)+0'].astype(float)\n",
    "    df_kpx['Power Generation(kW)+1'] = df_kpx['Power Generation(kW)+1'].astype(float)\n",
    "    df_kpx['Power Generation(kW)+2'] = df_kpx['Power Generation(kW)+2'].astype(float)\n",
    "    \n",
    "    n_splits = 5\n",
    "    score = main(df_kpx, n_splits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

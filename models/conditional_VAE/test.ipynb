{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "from vae_model import CVAEDataset, VAE\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "ts = time.time()\n",
    "new_data = pd.read_pickle(\"new_data_cvae.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = new_data.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(new_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CVAEDataset(train)\n",
    "test_dataset = CVAEDataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False,\n",
    "                          num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(recon_x, x, mean, log_var):\n",
    "    BCE = torch.nn.functional.mse_loss(\n",
    "        recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "    return (BCE + KLD) / x.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer_sizes = [train.shape[1]-1,train.shape[1]-5, train.shape[1]//2]\n",
    "decoder_layer_sizes = [train.shape[1]//2, train.shape[1]-5, train.shape[1]-1]\n",
    "latent_size = 7\n",
    "conditional = 1\n",
    "num_condition = len(train['location'].unique())\n",
    "learning_rate = 0.002\n",
    "epochs = 1\n",
    "print_every = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: VAE(\n",
      "  (encoder): Encoder(\n",
      "    (MLP): Sequential(\n",
      "      (L0): Linear(in_features=21, out_features=14, bias=True)\n",
      "      (BN0): BatchNorm1d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (A0): ReLU()\n",
      "      (L1): Linear(in_features=14, out_features=9, bias=True)\n",
      "      (BN1): BatchNorm1d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (A1): ReLU()\n",
      "    )\n",
      "    (linear_means): Linear(in_features=9, out_features=7, bias=True)\n",
      "    (linear_log_var): Linear(in_features=9, out_features=7, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (MLP): Sequential(\n",
      "      (L0): Linear(in_features=10, out_features=9, bias=True)\n",
      "      (A0): ReLU()\n",
      "      (BN0): BatchNorm1d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (L1): Linear(in_features=9, out_features=14, bias=True)\n",
      "      (A1): ReLU()\n",
      "      (BN1): BatchNorm1d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (L2): Linear(in_features=14, out_features=18, bias=True)\n",
      "      (sigmoid): Sigmoid()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Epoch 00/01 Batch 0000/3357, Loss    2.6593\n",
      "Epoch 00/01 Batch 0010/3357, Loss    2.3425\n",
      "Epoch 00/01 Batch 0020/3357, Loss    2.1663\n",
      "Epoch 00/01 Batch 0030/3357, Loss    1.9498\n",
      "Epoch 00/01 Batch 0040/3357, Loss    1.8315\n",
      "Epoch 00/01 Batch 0050/3357, Loss    1.7236\n",
      "Epoch 00/01 Batch 0060/3357, Loss    1.6136\n",
      "Epoch 00/01 Batch 0070/3357, Loss    1.4514\n",
      "Epoch 00/01 Batch 0080/3357, Loss    1.3156\n",
      "Epoch 00/01 Batch 0090/3357, Loss    1.1916\n",
      "Epoch 00/01 Batch 0100/3357, Loss    1.0941\n",
      "Epoch 00/01 Batch 0110/3357, Loss    1.0263\n",
      "Epoch 00/01 Batch 0120/3357, Loss    0.9363\n",
      "Epoch 00/01 Batch 0130/3357, Loss    0.9258\n",
      "Epoch 00/01 Batch 0140/3357, Loss    0.9278\n",
      "Epoch 00/01 Batch 0150/3357, Loss    0.8976\n",
      "Epoch 00/01 Batch 0160/3357, Loss    0.8922\n",
      "Epoch 00/01 Batch 0170/3357, Loss    0.8647\n",
      "Epoch 00/01 Batch 0180/3357, Loss    0.8748\n",
      "Epoch 00/01 Batch 0190/3357, Loss    0.8934\n",
      "Epoch 00/01 Batch 0200/3357, Loss    0.8253\n",
      "Epoch 00/01 Batch 0210/3357, Loss    0.8607\n",
      "Epoch 00/01 Batch 0220/3357, Loss    0.8482\n",
      "Epoch 00/01 Batch 0230/3357, Loss    0.8294\n",
      "Epoch 00/01 Batch 0240/3357, Loss    0.8049\n",
      "Epoch 00/01 Batch 0250/3357, Loss    0.8901\n",
      "Epoch 00/01 Batch 0260/3357, Loss    0.8986\n",
      "Epoch 00/01 Batch 0270/3357, Loss    0.8342\n",
      "Epoch 00/01 Batch 0280/3357, Loss    0.8689\n",
      "Epoch 00/01 Batch 0290/3357, Loss    0.8360\n",
      "Epoch 00/01 Batch 0300/3357, Loss    0.8309\n",
      "Epoch 00/01 Batch 0310/3357, Loss    0.7893\n",
      "Epoch 00/01 Batch 0320/3357, Loss    0.7892\n",
      "Epoch 00/01 Batch 0330/3357, Loss    0.8555\n",
      "Epoch 00/01 Batch 0340/3357, Loss    0.8048\n",
      "Epoch 00/01 Batch 0350/3357, Loss    0.8280\n",
      "Epoch 00/01 Batch 0360/3357, Loss    0.8601\n",
      "Epoch 00/01 Batch 0370/3357, Loss    0.8742\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-43335b951be7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vae = VAE(\n",
    "    encoder_layer_sizes=encoder_layer_sizes,\n",
    "    latent_size=latent_size,\n",
    "    decoder_layer_sizes=decoder_layer_sizes,\n",
    "    conditional=conditional,\n",
    "    num_labels=num_condition if conditional else 0,\n",
    "    num_condition=num_condition).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "\n",
    "logs = defaultdict(list)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for iteration, (x, y) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        if conditional:\n",
    "            recon_x, mean, log_var, z = vae(x, y)\n",
    "        else:\n",
    "            recon_x, mean, log_var, z = vae(x)\n",
    "\n",
    "        loss = loss_fn(recon_x.float(), x.float(), mean.float(), log_var.float())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        logs['loss'].append(loss.item())\n",
    "\n",
    "        if iteration % print_every == 0 or iteration == len(train_loader)-1:\n",
    "            print(\"Epoch {:02d}/{:02d} Batch {:04d}/{:d}, Loss {:9.4f}\".format(\n",
    "                epoch, epochs, iteration, len(train_loader)-1, loss.item()))\n",
    "\n",
    "            if conditional:\n",
    "                c = torch.arange(0, num_condition).long().unsqueeze(1)\n",
    "                x = vae.inference(n=c.size(0), c=c)\n",
    "            else:\n",
    "                x = vae.inference(n=num_condition)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = []\n",
    "    for iteration, (x,y) in enumerate(test_loader):\n",
    "\n",
    "        test_x = x.to(device)\n",
    "        test_y = y.to(device)\n",
    "        z += [vae.encode(test_x.to(device), test_y.to(device))]\n",
    "    z = torch.cat(z, dim=0)\n",
    "\n",
    "    z = z.mean(dim=0).cpu().numpy()\n",
    "    z = z[:,:2]\n",
    "    plt.scatter(x=test_z[:,0], y=test_z[:,1], c = test_y.cpu().numpy(), alpha=3)# , s='tab10')\n",
    "    plt.colorbar()\n",
    "#     plt.savefig('./plot/latent_space'+'.png',format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
